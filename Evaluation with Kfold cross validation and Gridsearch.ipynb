{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "#text preprocessing\n",
    "import string\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('./SMSCollection.csv',encoding='latin-1',error_bad_lines=False,header=None,\n",
    "                 sep='\\t',names=['label','text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5572 rows ,747 are spam and 4825 are ham\n"
     ]
    }
   ],
   "source": [
    "print(\"Out of {} rows ,{} are spam and {} are ham\".format(len(data),\n",
    "                                                          len(data[data['label']=='spam']),\n",
    "                                                          len(data[data['label']=='ham'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=nltk.corpus.stopwords.words(\"english\")\n",
    "st=nltk.SnowballStemmer(\"english\")\n",
    "#function to perform basic cleaning tasks\n",
    "def clean(text):\n",
    "    text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens=re.split('\\W+',text)\n",
    "    text=[st.stem(word) for word in tokens if word not in stopword]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tfidf for vectorizing data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(analyzer=clean)\n",
    "x_tfidf=tfidf.fit_transform(data['text'])\n",
    "#print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizers output a sparse matrix\n",
    "x_tfidfdf=pd.DataFrame(x_tfidf.toarray())\n",
    "x_tfidfdf.columns=tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  text_len\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        92\n",
       "1   ham                      Ok lar... Joking wif u oni...        24\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       128\n",
       "3   ham  U dun say so early hor... U c already then say...        39\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        49"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature engineering\n",
    "data['text_len']=data['text'].apply(lambda x:len(x)-x.count(\" \"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_feature</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>92</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>24</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>39</td>\n",
       "      <td>15.4</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  text_len  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        92   \n",
       "1   ham                      Ok lar... Joking wif u oni...        24   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       128   \n",
       "3   ham  U dun say so early hor... U c already then say...        39   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        49   \n",
       "\n",
       "   text_feature  punct%  \n",
       "0           9.8     9.8  \n",
       "1          25.0    25.0  \n",
       "2           4.7     4.7  \n",
       "3          15.4    15.4  \n",
       "4           4.1     4.1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create features for a % of the text which is punctuation\n",
    "\n",
    "def count_punct(text):\n",
    "    count=sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text)-text.count(\" \")),3)*100\n",
    "data['text_feature']=data['text'].apply(lambda x:count_punct(x))\n",
    "data['punct%']=data['text'].apply(lambda x:count_punct(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "bins=np.linspace(0,200,40)\n",
    "pyplot.hist(data[data['label']=='spam']['text_len'],bins,alpha=0.7,normed=True,label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['text_len'],bins,alpha=0.7,normed=True,label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#cross validation and rfc\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "xfeatures=pd.concat([data['text_len'],data['punct%'],pd.DataFrame(x_tfidf.toarray())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96950673, 0.97578475, 0.97037702, 0.96499102, 0.96947935])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf=RandomForestClassifier(n_jobs=-1)\n",
    "kfold=KFold(n_splits=5)\n",
    "cross_val_score(rf,xfeatures,data['label'],cv=kfold,scoring='accuracy',n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore random forest through the holdout set\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(xfeatures,data['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=50,max_depth=20,n_jobs=1)\n",
    "rf_model=rf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.056702808477674414, 'text_len'),\n",
       " (0.03297231197547871, 1816),\n",
       " (0.032970737797076585, 3152),\n",
       " (0.03272132749631371, 8144),\n",
       " (0.02870729984804416, 2047),\n",
       " (0.025914729846010825, 7385),\n",
       " (0.02075591505118663, 6774),\n",
       " (0.020428015305010018, 5755),\n",
       " (0.020188753487757206, 7498),\n",
       " (0.015587028414584885, 4826)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf_model.feature_importances_,xtrain.columns),reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.6918238993710691 0.8178438661710037 None\n"
     ]
    }
   ],
   "source": [
    "ypred=rf_model.predict(xtest)\n",
    "precision,recall,fscore,support=score(ytest,ypred,pos_label='spam',average='binary')\n",
    "print(precision,recall,fscore,support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest with grid search\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(xfeatures,data['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 10,depth 10---,precision 1.0,recall 0.341,accuracy 0.918\n",
      "est 10,depth 20---,precision 0.99,recall 0.703,accuracy 0.962\n",
      "est 10,depth 30---,precision 0.989,recall 0.674,accuracy 0.959\n",
      "est 10,depth None---,precision 0.991,recall 0.761,accuracy 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 1/3 [00:04<00:09,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 50,depth 10---,precision 1.0,recall 0.203,accuracy 0.901\n",
      "est 50,depth 20---,precision 1.0,recall 0.623,accuracy 0.953\n",
      "est 50,depth 30---,precision 0.99,recall 0.688,accuracy 0.961\n",
      "est 50,depth None---,precision 1.0,recall 0.768,accuracy 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|█████████████████████████████▎              | 2/3 [00:13<00:06,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 100,depth 10---,precision 1.0,recall 0.326,accuracy 0.917\n",
      "est 100,depth 20---,precision 1.0,recall 0.63,accuracy 0.954\n",
      "est 100,depth 30---,precision 1.0,recall 0.688,accuracy 0.961\n",
      "est 100,depth None---,precision 1.0,recall 0.826,accuracy 0.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 3/3 [00:30<00:00,  9.39s/it]\n"
     ]
    }
   ],
   "source": [
    "def train_rf(n_est,depth):\n",
    "    rf=RandomForestClassifier(n_estimators=n_est,max_depth=depth,n_jobs=-1)\n",
    "    rf_model=rf.fit(xtrain,ytrain)\n",
    "    ypred=rf_model.predict(xtest)\n",
    "    precision,recall,fscore,support=score(ytest,ypred,pos_label='spam',average='binary')\n",
    "    print('est {},depth {}---,precision {},recall {},accuracy {}'.format(n_est,depth,round(precision,3),round(recall,3),\n",
    "                                                                        round((ytest==ypred).sum()/len(ypred),3)))\n",
    "from tqdm import tqdm\n",
    "for n_est in tqdm([10,50,100]):\n",
    "    for depth in [10,20,30,None]:\n",
    "        train_rf(n_est,depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest evaluation with gridsearchCV(cross validation+grid search)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "x_tfidf_feat=pd.concat([data['text_len'],data['punct%'],pd.DataFrame(x_tfidf.toarray())])\n",
    "count_vect=CountVectorizer(analyzer=clean)\n",
    "x_count=count_vect.fit_transform(data['text'])\n",
    "\n",
    "x_tfidf_feat=pd.concat([data['text_len'],data['punct%'],pd.DataFrame(x_tfidf.toarray())],axis=1)\n",
    "x_count_feat=pd.concat([data['text_len'],data['punct%'],pd.DataFrame(x_tfidf.toarray())],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "rf=RandomForestClassifier()\n",
    "param={'n_estimators':[10,150,300],\n",
    "      'max_depth':[30,60,90,None]}\n",
    "\n",
    "gs=GridSearchCV(rf,param,cv=5,n_jobs=-1)\n",
    "gs_fit=gs.fit(x_tfidf_feat,data['label'])#do x_count_feat also\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score',ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boosting grid search\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "#build own grid search\n",
    "#random forest with grid search\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(xfeatures,data['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/3 [00:00<?, ?it/s]C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 50,depth 3,lr 0.01,---,precision 0.0,recall 0.0,accuracy 0.875\n",
      "est 50,depth 3,lr 0.1,---,precision 0.871,recall 0.727,accuracy 0.952\n",
      "est 50,depth 3,lr 1,---,precision 0.842,recall 0.806,accuracy 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 50,depth 7,lr 0.01,---,precision 0.0,recall 0.0,accuracy 0.875\n",
      "est 50,depth 7,lr 0.1,---,precision 0.866,recall 0.791,accuracy 0.959\n",
      "est 50,depth 7,lr 1,---,precision 0.806,recall 0.835,accuracy 0.954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 50,depth 11,lr 0.01,---,precision 0.0,recall 0.0,accuracy 0.875\n",
      "est 50,depth 11,lr 0.1,---,precision 0.852,recall 0.827,accuracy 0.961\n",
      "est 50,depth 11,lr 1,---,precision 0.812,recall 0.842,accuracy 0.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 50,depth 15,lr 0.01,---,precision 0.0,recall 0.0,accuracy 0.875\n",
      "est 50,depth 15,lr 0.1,---,precision 0.853,recall 0.835,accuracy 0.961\n",
      "est 50,depth 15,lr 1,---,precision 0.811,recall 0.835,accuracy 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|████████████▎                        | 1/3 [4:39:17<9:18:34, 16757.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 100,depth 3,lr 0.01,---,precision 0.945,recall 0.374,accuracy 0.919\n",
      "est 100,depth 3,lr 0.1,---,precision 0.884,recall 0.77,accuracy 0.959\n",
      "est 100,depth 3,lr 1,---,precision 0.85,recall 0.813,accuracy 0.959\n",
      "est 100,depth 7,lr 0.01,---,precision 0.863,recall 0.633,accuracy 0.942\n",
      "est 100,depth 7,lr 0.1,---,precision 0.873,recall 0.842,accuracy 0.965\n",
      "est 100,depth 7,lr 1,---,precision 0.832,recall 0.82,accuracy 0.957\n",
      "est 100,depth 11,lr 0.01,---,precision 0.845,recall 0.705,accuracy 0.947\n",
      "est 100,depth 11,lr 0.1,---,precision 0.871,recall 0.827,accuracy 0.963\n",
      "est 100,depth 11,lr 1,---,precision 0.818,recall 0.842,accuracy 0.957\n",
      "est 100,depth 15,lr 0.01,---,precision 0.835,recall 0.763,accuracy 0.952\n",
      "est 100,depth 15,lr 0.1,---,precision 0.86,recall 0.842,accuracy 0.963\n",
      "est 100,depth 15,lr 1,---,precision 0.814,recall 0.849,accuracy 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████            | 2/3 [15:38:23<6:33:13, 23593.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est 150,depth 3,lr 0.01,---,precision 0.848,recall 0.561,accuracy 0.933\n",
      "est 150,depth 3,lr 0.1,---,precision 0.883,recall 0.763,accuracy 0.958\n",
      "est 150,depth 3,lr 1,---,precision 0.844,recall 0.82,accuracy 0.959\n",
      "est 150,depth 7,lr 0.01,---,precision 0.862,recall 0.676,accuracy 0.946\n",
      "est 150,depth 7,lr 0.1,---,precision 0.872,recall 0.835,accuracy 0.964\n",
      "est 150,depth 7,lr 1,---,precision 0.827,recall 0.827,accuracy 0.957\n",
      "est 150,depth 11,lr 0.01,---,precision 0.843,recall 0.734,accuracy 0.95\n",
      "est 150,depth 11,lr 0.1,---,precision 0.853,recall 0.835,accuracy 0.961\n",
      "est 150,depth 11,lr 1,---,precision 0.825,recall 0.849,accuracy 0.959\n"
     ]
    }
   ],
   "source": [
    "def train_gb(est,depth,lr):\n",
    "    gb=GradientBoostingClassifier(n_estimators=n_est,max_depth=depth,learning_rate=lr)\n",
    "    gb_model=gb.fit(xtrain,ytrain)\n",
    "    ypred=gb_model.predict(xtest)\n",
    "    precision,recall,fscore,support=score(ytest,ypred,pos_label='spam',average='binary')\n",
    "    print('est {},depth {},lr {},---,precision {},recall {},accuracy {}'.format(n_est,depth,lr,round(precision,3),round(recall,3),\n",
    "                                                                        round((ytest==ypred).sum()/len(ypred),3)))\n",
    "\n",
    "from tqdm import tqdm\n",
    "for n_est in tqdm([50,100,150]):\n",
    "    for depth in [3,7,11,15]:\n",
    "        for lr in[0.01,0.1,1]:\n",
    "            train_gb(n_est,depth,lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient boosting with gridsearch CV\n",
    "gb=GradientBoostingClassifier()\n",
    "param={'n_estimators':[100,150],\n",
    "      'max_depth':[7,11,15],\n",
    "      'learning_rate':[0.1]\n",
    "      }\n",
    "\n",
    "gs=GridSearchCV(gb,param,cv=5,n_jobs=-1)\n",
    "gs_fit=gs.fit(x_tfidf_feat,data['label'])#do x_count_feat also\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score',ascending=False)[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
